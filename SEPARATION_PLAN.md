# Tab2Graph Webdemo â†’ Tabular Chat Predictor
## Separation Plan & Implementation Guide

### ðŸŽ¯ Project Overview

**Objective**: Separate the Tab2Graph webdemo into a clean, standalone "tabular-chat-predictor" repository that maintains all current functionality while being completely independent from the main tab2graph codebase.

**Target Repository**: `tabular-chat-predictor`
**Current Source**: `tab2graph/webdemo/`

---

## ðŸ“‹ Architecture Analysis

### Current Webdemo Structure
```
tab2graph/webdemo/
â”œâ”€â”€ __init__.py                    # Package initialization
â”œâ”€â”€ agents/                        # LLM agents and orchestration
â”‚   â”œâ”€â”€ base_agent.py             # Base agent functionality
â”‚   â”œâ”€â”€ coding_assistant.py       # Interactive coding with REPL
â”‚   â”œâ”€â”€ llm_client.py             # LiteLLM integration
â”‚   â”œâ”€â”€ orchestrator.py           # Main conversation orchestrator
â”‚   â””â”€â”€ predicting_assistant.py   # ML prediction workflows
â”œâ”€â”€ compute/                       # Core computational engine
â”‚   â”œâ”€â”€ compute_engine.py         # Main computational interface
â”‚   â”œâ”€â”€ dataset_loader.py         # DFS dataset loading
â”‚   â”œâ”€â”€ repl_manager.py           # IPython REPL management
â”‚   â”œâ”€â”€ tabpfn_manager.py         # TabPFN model management
â”‚   â””â”€â”€ tabpfn_predictor.py       # TabPFN prediction interface
â”œâ”€â”€ core/                          # Core protocols and tools
â”‚   â”œâ”€â”€ protocols.py              # Abstract interfaces
â”‚   â”œâ”€â”€ tool.py                   # Tool creation utilities
â”‚   â””â”€â”€ tool_registry.py         # Tool registration system
â”œâ”€â”€ frontends/                     # User interfaces
â”‚   â”œâ”€â”€ console_frontend.py       # Command-line interface
â”‚   â””â”€â”€ streamlit_frontend.py     # Web-based interface
â”œâ”€â”€ interfaces/                    # Interface implementations
â”‚   â”œâ”€â”€ console_interface.py      # Console UI implementation
â”‚   â””â”€â”€ streamlit_interface.py    # Streamlit UI implementation
â”œâ”€â”€ templates/                     # Jinja2 prompt templates
â”‚   â”œâ”€â”€ orchestrator_prompt.jinja
â”‚   â””â”€â”€ predictive_query_prompt.jinja
â”œâ”€â”€ icl_examples/                  # In-context learning examples
â”‚   â”œâ”€â”€ amazon/                   # Amazon dataset examples
â”‚   â””â”€â”€ stack/                    # StackExchange examples
â”œâ”€â”€ coding_assistant_prompt.jinja # Coding assistant template
â”œâ”€â”€ schema_to_mermaid.py          # Schema visualization
â””â”€â”€ training_labels_prompt.jinja  # Training label generation
```

### Key Features to Preserve
- âœ… **ML Predictions**: Complete TabPFN-based prediction pipeline
- âœ… **SHAP Explanations**: Model interpretability with SHAP
- âœ… **Data Exploration**: Interactive coding assistant with persistent REPL
- âœ… **Dual Interfaces**: Both Streamlit web UI and console interface
- âœ… **LLM Orchestration**: Intelligent tool selection and workflow management
- âœ… **Streaming Responses**: Real-time LLM response streaming

### Dependencies Analysis
**Current Dependencies** (from webdemo code analysis):
- **Core ML**: `tabpfn`, `tabpfn-extensions`, `shap`, `scikit-learn`
- **LLM Integration**: `litellm`, `jinja2`, `pydantic`
- **UI Components**: `streamlit`, `streamlit-mermaid`, `rich`, `prompt-toolkit`
- **Data Processing**: `pandas`, `numpy`, `duckdb`, `yaml`
- **Python Environment**: `ipython`, `matplotlib`, `seaborn`
- **Standard Libraries**: `pathlib`, `datetime`, `json`, `uuid`, `logging`

**Estimated Clean Dependencies**: ~15-20 packages (vs current 250+)

---

## ðŸ—ï¸ Target Repository Structure

```
tabular-chat-predictor/
â”œâ”€â”€ tabular_chat_predictor/        # Main package (no src/ needed)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents/                    # LLM agents (orchestrator, coding, predicting)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â”œâ”€â”€ coding_assistant.py
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â””â”€â”€ predicting_assistant.py
â”‚   â”œâ”€â”€ compute/                   # Core computation engine and TabPFN integration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ compute_engine.py
â”‚   â”‚   â”œâ”€â”€ dataset_loader.py
â”‚   â”‚   â”œâ”€â”€ repl_manager.py
â”‚   â”‚   â”œâ”€â”€ tabpfn_manager.py
â”‚   â”‚   â””â”€â”€ tabpfn_predictor.py
â”‚   â”œâ”€â”€ core/                      # Protocols, tools, and base classes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ protocols.py
â”‚   â”‚   â”œâ”€â”€ tool.py
â”‚   â”‚   â””â”€â”€ tool_registry.py
â”‚   â”œâ”€â”€ frontends/                 # Streamlit and console frontends
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ console_frontend.py
â”‚   â”‚   â””â”€â”€ streamlit_frontend.py
â”‚   â”œâ”€â”€ interfaces/                # UI and logging implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ console_interface.py
â”‚   â”‚   â””â”€â”€ streamlit_interface.py
â”‚   â”œâ”€â”€ templates/                 # Jinja2 prompt templates
â”‚   â”‚   â”œâ”€â”€ orchestrator_prompt.jinja
â”‚   â”‚   â””â”€â”€ predictive_query_prompt.jinja
â”‚   â”œâ”€â”€ icl_examples/              # In-context learning examples
â”‚   â”‚   â”œâ”€â”€ icl_examples.json
â”‚   â”‚   â”œâ”€â”€ amazon/
â”‚   â”‚   â””â”€â”€ stack/
â”‚   â”œâ”€â”€ coding_assistant_prompt.jinja
â”‚   â”œâ”€â”€ schema_to_mermaid.py
â”‚   â””â”€â”€ training_labels_prompt.jinja
â”œâ”€â”€ datasets/                      # Sample datasets (Amazon, StackExchange)
â”‚   â”œâ”€â”€ demo/
â”‚   â”‚   â”œâ”€â”€ rel-amazon-input/
â”‚   â”‚   â”‚   â”œâ”€â”€ __dfs__/
â”‚   â”‚   â”‚   â””â”€â”€ metadata.yaml
â”‚   â”‚   â””â”€â”€ rel-stack-input/
â”‚   â”‚       â”œâ”€â”€ __dfs__/
â”‚   â”‚       â””â”€â”€ metadata.yaml
â”œâ”€â”€ examples/                      # Usage examples and scripts
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ streamlit_demo.py
â”‚   â””â”€â”€ console_demo.py
â”œâ”€â”€ README.md                      # Comprehensive setup and usage guide
â”œâ”€â”€ pyproject.toml                 # Package configuration
â”œâ”€â”€ requirements.txt               # Clean dependency list
â””â”€â”€ .gitignore                     # Git ignore file
```

---

## ðŸ“ Implementation Plan (10 Steps)

### Step 1: Analyze Dependencies and Create Clean List
**Status**: Pending
**Tasks**:
- Extract essential dependencies from current requirements.txt (250+ â†’ ~15-20)
- Identify webdemo-specific packages vs tab2graph-specific packages
- Create minimal requirements.txt with only necessary packages

**Key Dependencies**:
```
# Core ML
tabpfn>=2.0.9
tabpfn-extensions>=0.0.4
shap>=0.48.0
scikit-learn>=1.6.0

# LLM Integration  
litellm>=1.74.0
jinja2>=3.1.6
pydantic>=2.11.7

# UI Components
streamlit>=1.46.0
streamlit-mermaid>=0.3.0
rich>=14.0.0
prompt-toolkit

# Data Processing
pandas>=2.2.3
numpy>=2.2.0
duckdb>=1.3.0
pyyaml>=6.0.2

# Python Environment
ipython
matplotlib>=3.10.0
seaborn>=0.12.0
```

### Step 2: Design New Repository Structure
**Status**: Pending
**Tasks**:
- Create directory structure for `tabular-chat-predictor`
- Plan package layout under `tabular_chat_predictor/` (no src/ needed)
- Design datasets and examples directories

### Step 3: Create Project Configuration Files
**Status**: Pending
**Tasks**:
- Create `pyproject.toml` with package metadata and dependencies
- Create clean `requirements.txt`
- Create `.gitignore` file
- Set up package entry points for CLI usage

**pyproject.toml Structure**:
```toml
[project]
name = "tabular-chat-predictor"
version = "0.1.0"
description = "Interactive LLM-driven predictive analytics for tabular data"
dependencies = [
    # Clean dependency list from Step 1
]

[project.scripts]
tabular-chat-predictor = "tabular_chat_predictor.frontends.console_frontend:main"
```

### Step 4: Copy and Refactor Core Code
**Status**: Pending
**Tasks**:
- Copy all files from `tab2graph/webdemo/` to new structure
- Remove any tab2graph-specific code or references
- Ensure all functionality is preserved

**Files to Copy**:
- All `.py` files from webdemo directory
- All `.jinja` template files
- All ICL examples and data

### Step 5: Update Import Statements
**Status**: Pending
**Tasks**:
- Replace all `from tab2graph.webdemo.*` imports with `from tabular_chat_predictor.*`
- Update relative imports to match new package structure
- Ensure no remaining references to tab2graph package

**Import Changes**:
```python
# Before
from tab2graph.webdemo.compute.compute_engine import ComputeEngine
from tab2graph.webdemo.agents.predicting_assistant import PredictingAssistant

# After  
from tabular_chat_predictor.compute.compute_engine import ComputeEngine
from tabular_chat_predictor.agents.predicting_assistant import PredictingAssistant
```

### Step 6: Create Sample Datasets Directory
**Status**: Pending
**Tasks**:
- Copy Amazon and StackExchange sample datasets
- Ensure datasets are in correct DFS format with `__dfs__/` directories
- Include metadata.yaml files for each dataset
- Verify datasets work with existing data loaders

**Dataset Structure**:
```
datasets/demo/
â”œâ”€â”€ rel-amazon-input/
â”‚   â”œâ”€â”€ __dfs__/
â”‚   â”‚   â”œâ”€â”€ customer.npz
â”‚   â”‚   â””â”€â”€ product.npz
â”‚   â””â”€â”€ metadata.yaml
â””â”€â”€ rel-stack-input/
    â”œâ”€â”€ __dfs__/
    â”‚   â”œâ”€â”€ users.npz
    â”‚   â””â”€â”€ posts.npz
    â””â”€â”€ metadata.yaml
```

### Step 7: Update Configuration and Template Files
**Status**: Pending
**Tasks**:
- Update dataset paths in frontend configurations
- Modify Jinja2 templates to remove tab2graph references
- Update example queries and ICL examples
- Ensure all hardcoded paths are relative to new structure

**Configuration Updates**:
- Update dataset paths in `streamlit_frontend.py`
- Modify template loading paths in agents
- Update ICL example file paths

### Step 8: Create Comprehensive README
**Status**: Pending
**Tasks**:
- Write installation instructions
- Document usage examples for both interfaces
- Explain dataset format requirements
- Provide troubleshooting guide

**README Sections**:
```markdown
# Tabular Chat Predictor

## Installation
## Quick Start
## Features
## Usage Examples
## Dataset Format
## Configuration
## Troubleshooting
```

### Step 9: Add Example Scripts
**Status**: Pending
**Tasks**:
- Create `examples/basic_usage.py` - Simple prediction example
- Create `examples/streamlit_demo.py` - Streamlit interface demo
- Create `examples/console_demo.py` - Console interface demo
- Add Jupyter notebook examples

### Step 10: Validate Project Works
**Status**: Pending
**Tasks**:
- Test that the project can be launched with a single command
- Verify basic functionality works end-to-end

**Validation Checklist**:
- [ ] Project installs successfully with `pip install -e .`
- [ ] Streamlit interface launches with single command
- [ ] Console interface launches with single command
- [ ] Basic prediction workflow completes successfully

---

## ðŸš€ Expected Benefits

### For Users
- **Easy Installation**: Simple `pip install tabular-chat-predictor`
- **Self-Contained**: No dependency on large tab2graph framework
- **Immediate Usability**: Working examples and sample datasets included
- **Clear Documentation**: Comprehensive setup and usage guide

### For Developers
- **Clean Codebase**: Focused, minimal dependencies
- **Maintainable**: Simple structure easy to understand and modify
- **Extensible**: Well-architected for adding new features
- **Independent**: No external framework dependencies

### Technical Advantages
- **Minimal Dependencies**: ~15-20 packages vs 250+ in original
- **Fast Installation**: Quick pip install without heavy dependencies
- **Portable**: Easy to deploy in different environments
- **Focused**: Purpose-built for tabular data chat prediction

---

## ðŸŽ¯ Success Criteria

The separation will be considered successful when:

1. **Complete Independence**: No imports from tab2graph package
2. **Full Functionality**: All current features work identically
3. **Easy Setup**: One-command installation and startup
4. **Working Examples**: All provided examples run successfully
5. **Sample Data**: Included datasets work out of the box
6. **Clear Documentation**: Users can get started quickly from README

---

## ðŸ“… Implementation Timeline

**Estimated Time**: 1-2 days for core implementation
**Priority Order**: Steps 1-5 are critical path, Steps 6-10 can be done in parallel

This plan maintains the excellent architecture of the current webdemo while making it completely standalone and accessible to users who want a powerful tabular data chat predictor without the full tab2graph framework.